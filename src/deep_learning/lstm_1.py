# import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

# Please use python 3.5 or above
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential, Model
from keras.backend import argmax
from keras.layers import Dense, Embedding, LSTM, LeakyReLU, Concatenate, Input, Bidirectional, Conv1D, MaxPooling1D, Reshape, MaxPooling2D, Flatten, Dropout, Conv2D, MaxPooling2D
from keras import optimizers
from nltk.tokenize import TweetTokenizer
from keras.models import load_model
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from keras.metrics import  categorical_accuracy, binary_accuracy
from spellchecker import SpellChecker
from keras.losses import categorical_crossentropy
import json, argparse, os
import re
import io
import sys, sklearn
import tensorflow as tf
from emoji import UNICODE_EMOJI

# Path to training and testing data file. This data can be downloaded from a link, details of which will be provided.
trainDataPath = ""
testDataPath = ""
validDataPath = ""
# Output file that will be generated. This file can be directly submitted.
solutionPath = ""
# Path to directory where GloVe file is saved.
gloveDir = ""
# Path to GloVe embedding matrix
embeddingMatrixGlovePath = ""
# Path to directory where sswe file is saved
ssweDir = ""
# Path to sswe embedding matrix
embeddingMatrixSSWEPath = ""

NUM_FOLDS = None                   # Value of K in K-fold Cross Validation
NUM_CLASSES = None                 # Number of classes - Happy, Sad, Angry, Others
MAX_NB_WORDS = None                # To set the upper limit on the number of tokens extracted using keras.preprocessing.text.Tokenizer
MAX_SEQUENCE_LENGTH = None         # All sentences having lesser number of words than this will be padded
EMBEDDING_DIM_GLOVE = None         # The dimension of the GloVe word embeddings
EMBEDDING_DIM_SSWE = None          # The dimension of the SSWE word embeddings
BATCH_SIZE = None                  # The batch size to be chosen for training the model.
LSTM_DIM = None                    # The dimension of the representations learnt by the LSTM model
DROPOUT = None                     # Fraction of the units to drop for the linear transformation of the inputs. Ref - https://keras.io/layers/recurrent/
NUM_EPOCHS = None                  # Number of epochs to train a model for


label2emotion = {0:"others", 1:"happy", 2: "sad", 3:"angry"}
emotion2label = {"others":0, "happy":1, "sad":2, "angry":3}

emoji2emoticons = {'üòë': ':|', 'üòñ': ':(', 'üòØ': ':o', 'üòù': ':p', 'üòê': ':|',
                'üòà': ':)', 'üôÅ': ':(', 'üòé': ':)', 'üòû': ':(', '‚ô•Ô∏è': '<3', 'üíï': 'love',
                'üòÄ': ':d', 'üò¢': ":(", 'üëç': 'ok', 'üòá': ':)', 'üòú': ':p',
                'üíô': 'love', '‚òπÔ∏è': ':(', 'üòò': ':)', 'ü§î': 'hmm', 'üò≤': ':o',
                'üôÇ': ':)', '\U0001f923': ':d', 'üòÇ': ':d', 'üëø': ':(', 'üòõ': ':p',
                'üòâ': ';)', 'ü§ì': '8-)'}


def clean_word(word):
    """Takes a word, separates emoticons, corrects spelling
    :param word: a string
    :return: list of emoticons, words with correct spellings
    """
    final_list = []
    temp_word = ""
    for k in range(len(word)):
        if word[k] in UNICODE_EMOJI:
            # check if word[k] is an emoji
            if(word[k] in emoji2emoticons):
                final_list.append(emoji2emoticons[word[k]])
        else:
            temp_word = temp_word + word[k]

    if len(temp_word) > 0:
        # "righttttttt" -> "right"
        temp_word = re.sub(r'(.)\1+', r'\1\1', temp_word)
        # correct spelling
        spell = SpellChecker()
        temp_word = spell.correction(temp_word)
        # lemmatize
        temp_word = WordNetLemmatizer().lemmatize(temp_word)
        final_list.append(temp_word)

    return final_list


def preprocessData(dataFilePath, mode):
    """Load data from a file, process and return indices, conversations and labels in separate lists
    Input:
        dataFilePath : Path to train/test file to be processed
        mode : "train" mode returns labels. "test" mode doesn't return labels.
    Output:
        indices : Unique conversation ID list
        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag
        labels : [Only available in "train" mode] List of labels
    """
    indices = []
    conversations = []
    labels = []
    with io.open(dataFilePath, encoding="utf8") as finput:
        finput.readline()
        for line in finput:
            # Convert multiple instances of . ? ! , to single instance
            # okay...sure -> okay . sure
            # okay???sure -> okay ? sure
            # Add whitespace around such punctuation
            # okay!sure -> okay ! sure
            repeatedChars = ['.', '?', '!', ',']
            for c in repeatedChars:
                lineSplit = line.split(c)
                while True:
                    try:
                        lineSplit.remove('')
                    except:
                        break
                cSpace = ' ' + c + ' '
                line = cSpace.join(lineSplit)

            line = line.strip().split('\t')
            if mode == "train":
                # Train data contains id, 3 turns and label
                label = emotion2label[line[4]]
                labels.append(label)

            conv = ' <eos> '.join(line[1:4])

            # Remove any duplicate spaces
            duplicateSpacePattern = re.compile(r'\ +')
            conv = re.sub(duplicateSpacePattern, ' ', conv)

            indices.append(int(line[0]))
            conversations.append(conv.lower())

    if mode == "train":
        return indices, conversations, labels
    else:
        return indices, conversations


def getMetrics(ground, predictions):
    """Given predicted labels and the respective ground truth labels, display some metrics
    Input: shape [# of samples, NUM_CLASSES]
        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class
        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]
    Output:
        accuracy : Average accuracy
        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001
        microRecall : Recall calculated on a micro level
        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification
    """
    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]
    discretePredictions = tf.one_hot(predictions, depth=4)
    # ground = tf.constant(ground)#,dtype=tf.float32)
    # if tf.equal(discretePredictions.shape, ground.shape) is True:
    #     print ('All well here')
    truePositives = np.sum((discretePredictions*ground), axis=0)
    falsePositives = np.sum(tf.clip_by_value(discretePredictions - ground, 0.0, 1.0), axis=0)
    falseNegatives = np.sum(tf.clip_by_value(ground-discretePredictions, 0.0, 1.0), axis=0)

    # print("True Positives per class : ", truePositives)
    # print("False Positives per class : ", falsePositives)
    # print("False Negatives per class : ", falseNegatives)

    # # ------------- Macro level calculation ---------------
    # macroPrecision = 0
    # macroRecall = 0
    # # We ignore the "Others" class during the calculation of Precision, Recall and F1
    # for c in range(1, NUM_CLASSES):
    #     precision = truePositives[c] / (truePositives[c] + falsePositives[c])
    #     macroPrecision += precision
    #     recall = truePositives[c] / (truePositives[c] + falseNegatives[c])
    #     macroRecall += recall
    #     if tf.greater(precision+recall,0) is True:
    #         f1 = ( 2 * recall * precision ) / (precision + recall)
    #     else:
    #         f1 = 0
    #     #print("Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f" % (label2emotion[c], precision, recall, f1))
    #
    # macroPrecision /= 3
    # macroRecall /= 3
    #
    # if tf.greater(macroPrecision+macroRecall, 0) is True:
    #     macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall)
    # else:
    #     macroF1 = 0
    # #print("Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f" % (macroPrecision, macroRecall, macroF1))
    #
    # ------------- Micro level calculation ---------------
    truePositives = tf.reduce_sum(truePositives[1:])
    falsePositives = tf.reduce_sum(falsePositives[1:])
    falseNegatives = tf.reduce_sum(falseNegatives[1:])

    # print("Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d" % (truePositives, falsePositives, falseNegatives))

    microPrecision = tf.divide(truePositives, tf.add_n([truePositives, falsePositives]))
    microRecall = tf.divide(truePositives, tf.add_n([truePositives, falseNegatives]))

    # if tf.greater(tf.add_n([microPrecision, microRecall]), tf.constant(0.0)) is True:
    microF1 = tf.divide(tf.multiply(2.0, tf.multiply(microRecall, microPrecision)), tf.add_n([microPrecision, microRecall]))
    # else:
    #    microF1 = tf.constant(0.0, dtype=tf.float32)
    # -----------------------------------------------------

    # predictions = predictions.argmax(axis=1)
    # ground = ground.argmax(axis=1)
    # accuracy = np.mean(predictions==ground)

    # print("Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f" % (accuracy, microPrecision, microRecall, microF1))
    return("%.2f"%microF1)


def writeNormalisedData(dataFilePath, texts):
    """Write normalised data to a file
    Input:
        dataFilePath : Path to original train/test file that has been processed
        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.
    """
    normalisedDataFilePath = dataFilePath.replace(".txt", "_normalised.txt")
    with io.open(normalisedDataFilePath, 'w', encoding='utf8') as fout:
        with io.open(dataFilePath, encoding='utf8') as fin:
            fin.readline()
            for lineNum, line in enumerate(fin):
                line = line.strip().split('\t')
                normalisedLine = texts[lineNum].strip().split('<eos>')
                fout.write(line[0] + '\t')
                # Write the original turn, followed by the normalised version of the same turn
                fout.write(line[1] + '\t' + normalisedLine[0] + '\t')
                fout.write(line[2] + '\t' + normalisedLine[1] + '\t')
                fout.write(line[3] + '\t' + normalisedLine[2] + '\t')
                try:
                    # If label information available (train time)
                    fout.write(line[4] + '\n')
                except:
                    # If label information not available (test time)
                    fout.write('\n')


def getEmbeddingMatrix_glove(wordIndex, load=False):
    """Populate an embedding matrix using a word-index. If the word "happy" has an index 19,
       the 19th row in the embedding matrix should contain the embedding vector for the word "happy".
    Input:
        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser
        load: If set to True, then loads the embedding matrix from a file
    Output:
        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding
    """
    if load:
        embeddingMatrix = np.load(embeddingMatrixGlovePath)
        return embeddingMatrix

    embeddingsIndex = {}
    # Load the embedding vectors from ther GloVe file
    with io.open(os.path.join(gloveDir, 'glove.6B.100d.txt'), encoding="utf8") as f:
        for line in f:
            values = line.split()
            word = values[0]
            embeddingVector = np.asarray(values[1:], dtype='float32')
            embeddingsIndex[word] = embeddingVector
    
    print('Found %s word vectors.' % len(embeddingsIndex))

    bad_words_glove_1 = set([])
    bad_words_glove_2 = set([])
    counter=0
    
    # Minimum word index of any word is 1. 
    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM_GLOVE))
    for word, i in wordIndex.items():
        # print(".",)
        embeddingVector = embeddingsIndex.get(word)
        if embeddingVector is not None:
            # words not found in embedding index will be all-zeros.
            embeddingMatrix[i] = embeddingVector
        else:
            bad_words_glove_1.add(word)
            good_from_bad = clean_word(word)
            # sum all word vectors, obtained after clean_word
            temp_embedding_vector = np.zeros((1, EMBEDDING_DIM_GLOVE))
            for www in good_from_bad:
                eee = embeddingsIndex.get(www)
                if eee is not None:
                    temp_embedding_vector = temp_embedding_vector + eee
            if not temp_embedding_vector.all():
                bad_words_glove_2.add(word)
            embeddingMatrix[i] = temp_embedding_vector
        if(counter%1000==0):
            print(counter)
        counter+=1
    
    print("Bad words in GloVe 1 - %d"%len(bad_words_glove_1))
    print("Bad words in GloVe 2 - %d"%len(bad_words_glove_2))
    
#     np.save(embeddingMatrixGlovePath, embeddingMatrix)

    return embeddingMatrix


def getEmbeddingMatrix_sswe(wordIndex, load=False):
    """
    :param wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser
    :param load: If set to True, then loads the embedding matrix from a file
    :return: embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding
    """
    if load:
        embeddingMatrix = np.load(embeddingMatrixSSWEPath)
        return embeddingMatrix

    embeddingsIndex = {}
    # Load the embedding vectors from ther GloVe file
    with io.open(os.path.join(ssweDir, 'sswe-u.txt'), encoding="utf8") as f:
        for line in f:
            values = line.split()
            word = values[0]
            embeddingVector = np.asarray(values[1:], dtype='float32')
            embeddingsIndex[word] = embeddingVector

    print('Found %s word vectors.' % len(embeddingsIndex))

    bad_words_sswe_1 = set([])
    bad_words_sswe_2 = set([])
    counter=0

    # Minimum word index of any word is 1.
    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM_SSWE))
    for word, i in wordIndex.items():
        embeddingVector = embeddingsIndex.get(word)
        if embeddingVector is not None:
            # words not found in embedding index will be all-zeros.
            embeddingMatrix[i] = embeddingVector
        else:
            bad_words_sswe_1.add(word)
            good_from_bad = clean_word(word)
            # sum all word vectors, obtained after clean_word
            temp_embedding_vector = np.zeros((1, EMBEDDING_DIM_SSWE))
            for www in good_from_bad:
                eee = embeddingsIndex.get(www)
                if eee is not None:
                    temp_embedding_vector = temp_embedding_vector + eee
            if not temp_embedding_vector.all():
                bad_words_sswe_2.add(word)
            embeddingMatrix[i] = temp_embedding_vector
        if(counter%1000==0):
            print(counter)
        counter += 1

    print("Bad words in SSWE 1 - %d"%len(bad_words_sswe_1))
    print("Bad words in SSWE 2 - %d"%len(bad_words_sswe_2))
    
#     np.save(embeddingMatrixSSWEPath, embeddingMatrix)

    return embeddingMatrix


def buildModel(embeddingMatrix1, embeddingMatrix2):
    """Constructs the architecture of the model
    Input:
        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.
    Output:
        model : A basic LSTM model
    """
    main_input_1 = Input(shape=(100,), name='main_input_1')
    embeddingLayer1 = Embedding(embeddingMatrix1.shape[0],
                               EMBEDDING_DIM_GLOVE,
                               weights=[embeddingMatrix1],
                               input_length=MAX_SEQUENCE_LENGTH,
                               trainable=False) (main_input_1)
    model1 = LSTM(LSTM_DIM, dropout=DROPOUT, return_sequences=True) (embeddingLayer1)
    model1 = LSTM(LSTM_DIM, dropout=DROPOUT) (model1)
    
    main_input_2 = Input(shape=(100,), name='main_input_2')
    embeddingLayer2 = Embedding(embeddingMatrix2.shape[0],
                                EMBEDDING_DIM_SSWE,
                                weights=[embeddingMatrix2],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=False) (main_input_2)
    model2 = LSTM(LSTM_DIM, dropout=DROPOUT, return_sequences=True) (embeddingLayer2)
    model2 = LSTM(LSTM_DIM, dropout=DROPOUT) (model2)

    model3 = Concatenate() ([model1, model2])
    model3 = LeakyReLU(alpha=0.1) (model3)
    model3 = Dense(NUM_CLASSES, activation="softmax") (model3)
    model = Model([main_input_1, main_input_2], model3)
    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)
    model.compile(loss='categorical_crossentropy',
                  optimizer=rmsprop,
                  metrics=[categorical_accuracy])
    model.summary()

    return model


def cnnModel(embeddingMatrix1):
    """Constructs the architecture of the model
    Input:
    embeddingMatrix : The embedding matrix to be loaded in the embedding layer.
    Output:
    model : A basic CNN model
    """

    main_input_1 = Input(shape=(100,), name='main_input_1')
    embeddingLayer1 = Embedding(embeddingMatrix1.shape[0],
                               EMBEDDING_DIM_GLOVE,
                               weights=[embeddingMatrix1],
                               input_length=MAX_SEQUENCE_LENGTH,
                               trainable=False) (main_input_1)
    embeddingLayer1 = Reshape((100, EMBEDDING_DIM_GLOVE, 1)) (embeddingLayer1)
    model1 = Conv2D(NUM_FILTERS, kernel_size=(3,EMBEDDING_DIM_GLOVE), padding='same', activation='relu') (embeddingLayer1)
    model1 = Conv2D(NUM_FILTERS, kernel_size=(4,EMBEDDING_DIM_GLOVE), padding='same', activation='relu') (model1)
    model1 = Conv2D(NUM_FILTERS, kernel_size=(5,EMBEDDING_DIM_GLOVE), padding='same', activation='relu') (model1)
    model1 = MaxPooling2D(EMBEDDING_DIM_GLOVE) (model1)
    model1 = Flatten() (model1)
    model1 = Dense(NUM_CLASSES, activation="softmax") (model1)
    model = Model([main_input_1], model1)
    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)
    model.compile(loss='categorical_crossentropy',
                  optimizer=rmsprop,
                  metrics=[categorical_accuracy])
    model.summary()

    return model


def CNNModel(embeddingMatrix1):
    main_input = Input(shape=(100, ), name='main_input')
    embeddingLayer = Embedding(embeddingMatrix1.shape[0],
                               EMBEDDING_DIM_GLOVE,
                               weights=[embeddingMatrix1],
                               input_length=MAX_SEQUENCE_LENGTH,
                               trainable=False) (main_input)
    
    conv_blocks = []
    filter_sizes = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
    for fs in filter_sizes:
        conv = Conv1D(filters=NUM_FILTERS,
                  kernel_size=fs,  
                  padding='valid',  
                  strides=1,
                  activation='relu',
                  use_bias=True) (embeddingLayer)
        conv = Conv1D(filters=NUM_FILTERS,
                  kernel_size=fs,  
                  padding='valid',  
                  strides=1,
                  activation='relu',
                  use_bias=True) (conv)
        conv = MaxPooling1D(pool_size=2)(conv) 
        conv = Flatten()(conv) 
        conv_blocks.append(conv)
    
    concat1max = Concatenate()(conv_blocks)
    concat1max = Dropout(DROPOUT)(concat1max) 
    
    output_layer = Dense(50, activation='relu') (concat1max)
    output_layer = Dense(4, activation='softmax') (output_layer)
    
    model = Model(inputs=main_input, outputs=output_layer)
    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)
    model.compile(loss='categorical_crossentropy',
                  optimizer=rmsprop,
                  metrics=[categorical_accuracy])
    print(model.summary())
    
    return model


def microf1(ytrue, ypred):
    return sklearn.metrics.f1_score(ytrue, ypred, average='micro', labels=[1,2,3])


def all_metrics(ytrue, ypred):
    return sklearn.metrics.precision_recall_fscore_support(ytrue, ypred)


def helper(a):
    for i in range(len(a)):
        if a[i]==1:
            return i


def main():
    parser = argparse.ArgumentParser(description="Baseline Script for SemEval")
    parser.add_argument('-config', help='Config to read details', required=True)
    args = parser.parse_args()

    with open(args.config) as configfile:
        config = json.load(configfile)

    global trainDataPath, testDataPath, validDataPath, solutionPath, gloveDir, embeddingMatrixGlovePath, ssweDir
    global NUM_FOLDS, NUM_CLASSES, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM_GLOVE, EMBEDDING_DIM_SSWE
    global BATCH_SIZE, LSTM_DIM, DROPOUT, NUM_EPOCHS, LEARNING_RATE, embeddingMatrixSSWEPath, NUM_FILTERS

    trainDataPath = config["train_data_path"]
    testDataPath = config["test_data_path"]
    validDataPath = config["valid_data_path"]
    solutionPath = config["solution_path"]
    gloveDir = config["glove_dir"]
    embeddingMatrixGlovePath = config["embedding_matrix_glove_path"]
    ssweDir = config["sswe_dir"]
    embeddingMatrixSSWEPath = config["embedding_matrix_sswe_path"]

    NUM_FOLDS = config["num_folds"]
    NUM_CLASSES = config["num_classes"]
    MAX_NB_WORDS = config["max_nb_words"]
    MAX_SEQUENCE_LENGTH = config["max_sequence_length"]
    EMBEDDING_DIM_GLOVE = config["embedding_dim_glove"]
    EMBEDDING_DIM_SSWE = config["embedding_dim_sswe"]
    BATCH_SIZE = config["batch_size"]
    LSTM_DIM = config["lstm_dim"]
    
    DROPOUT = config["dropout"]
    LEARNING_RATE = config["learning_rate"]
    NUM_EPOCHS = config["num_epochs"]
    NUM_FILTERS = config["num_filters"]

    print("Processing training data...")
    new_trainText=[]
    tweet_tokenizer = TweetTokenizer()
    trainIndices, trainTexts, labels = preprocessData(trainDataPath, mode="train")
    for i in range(len(trainTexts)):
        tokens = tweet_tokenizer.tokenize(trainTexts[i])
        sent = ' '.join(tokens)
        new_trainText.append(sent)
    print ('Size of trainText = ', len(trainTexts))
    print ('Size of new_trainText = ', len(new_trainText))

    print("\nProcessing Validation data...")
    new_validText=[]
    validIndices, validTexts, validlabels = preprocessData(validDataPath, mode="train")
    for i in range(len(validTexts)):
        tokens = tweet_tokenizer.tokenize(validTexts[i])
        sent = ' '.join(tokens)
        new_validText.append(sent)
    print ('Size of validText = ', len(validTexts))
    print ('Size of new_validText = ', len(new_validText))
    
    print("\nProcessing Test data...")
    new_testText=[]
    testIndices, testTexts, testlabels = preprocessData(testDataPath, mode="train")
    for i in range(len(testTexts)):
        tokens = tweet_tokenizer.tokenize(testTexts[i])
        sent = ' '.join(tokens)
        new_testText.append(sent)
    print ('Size of testText = ', len(testTexts))
    print ('Size of new_testText = ', len(new_testText))

    # Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable
    # writeNormalisedData(trainDataPath, trainTexts)

    print("Extracting tokens...")
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(new_trainText)
    trainSequences = tokenizer.texts_to_sequences(new_trainText)
    validSequences = tokenizer.texts_to_sequences(new_validText)
    testSequences = tokenizer.texts_to_sequences(new_testText)

    wordIndex = tokenizer.word_index
    print("Found %s unique tokens." % len(wordIndex))

    print("Populating embedding matrix...")
    embeddingMatrix_glove = getEmbeddingMatrix_glove(wordIndex, True)
    embeddingMatrix_sswe = getEmbeddingMatrix_sswe(wordIndex, True)

    data = pad_sequences(trainSequences, maxlen=MAX_SEQUENCE_LENGTH)
    labels = to_categorical(np.asarray(labels))
    print("Shape of training data tensor: ", data.shape)
    print("Shape of label tensor: ", labels.shape)

    data_valid = pad_sequences(validSequences, maxlen=MAX_SEQUENCE_LENGTH)
    labels_valid = to_categorical(np.asarray(validlabels))
    print("Shape of Validation data tensor: ", data_valid.shape)
    print("Shape of validation label tensor: ", labels_valid.shape)
    
    data_test = pad_sequences(testSequences, maxlen=MAX_SEQUENCE_LENGTH)
    labels_test = to_categorical(np.asarray(testlabels))
    print("Shape of Validation data tensor: ", data_test.shape)
    print("Shape of validation label tensor: ", labels_test.shape)

    # Randomize Training data
    np.random.shuffle(trainIndices)
    data = data[trainIndices]
    labels = labels[trainIndices]
    
    # Randomize Validation data
    np.random.shuffle(validIndices)
    data_valid = data_valid[validIndices]
    labels_valid = labels_valid[validIndices]
    
    model = buildModel(embeddingMatrix_glove, embeddingMatrix_sswe)
    model.fit([data, data], labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)
    
    predictions = model.predict([data_valid])
    predictions = predictions.argmax(axis=1)
    
    final_predictions = model.predict([data_test])
    final_predictions = final_predictions.argmax(axis=1)
    
    f1score_dev=microf1(labels_valid.argmax(axis=1), predictions)
    
    f1score_test=microf1(labels_test.argmax(axis=1), final_predictions)
    
    all_metrics_dev=all_metrics(labels_valid.argmax(axis=1), predictions)
                    
    all_metrics_test=all_metrics(labels_test.argmax(axis=1), final_predictions)
                    
    print(f1score_dev)
    print(f1score_test)
    print(all_metrics_dev)
    print(all_metrics_test)


if __name__ == '__main__':
    main()

